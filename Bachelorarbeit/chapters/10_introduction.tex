\chapter{Introduction}\label{chap:intro}
% This thesis is about Bayesian optimization. The main topic is the derivation of an upper bound on the chance of further improvement in such a scheme.
% \textcolor{red}{Satz rauslassen?}
% % In order to discuss Bayesian optimization, the first half of this thesis will be dedicated to the neccessary probability theory foundations, some stochastical tools and Gaussian process regression.
%
%
\section{Topic of this Thesis}
% \section*{Optimization}
Let us start by looking at a generic optimization problem. That is, we have some function \( f \colon \mathcal{X} \to \mathbb{R} \) on a suitable space $\mathcal{X}$, and we want to find a minimizer
\[
    x^{*} \in \argmin_{x \in \mathcal{X}} f(x).
\]
An example of such a function is shown in \cref{fig:optimization_intro}.
One simple way to approach this problem would be to start checking points randomly and remembering the lowest point we encountered. 
For finite spaces $\mathcal{X}$ this is sometimes feasible, but quickly runs into limitations if the number of elements gets too large.
% 
An example of a more refined strategy would be to use information of a lower bound on the function, if we have one. 
This lets us rule out areas of search, where the lower bound is already higher than a value we have previously encountered, and subsequently only look in areas, in which improvement is possible. 
This method is called branch and bound~\cite{lawler1966branch}.
Although this is an improvement on the previous method, it still does not guarantee, that we will find the global minimum.
% 
But it does show a common theme in optimization of using all the availaible information about the space and the objective function to our benefit. Is the function differentiable? Linear? Is the space discrete? Is it continous, but still finite dimensional? These, among many others, are restrictions with specific approaches taylored to them. The approach discussed in this thesis is no different. 
\begin{figure}[H]
    \centering
    \input{figures/optimization_intro.pgf}
    \caption{A first optimization example.}
    \label{fig:optimization_intro}
\end{figure}


The setup is as follows: We have a function that is possibly expensive to evaluate and behaves randomly, but the structure of that randomness can be well quantified locally.
``Structure of randomness?'' This is most easily explained with a picture. 
The data for the three pictures in \cref{fig:random_fields_intro.png} was randomly generated by three different random processes. 
They are all random, but don't share the same structure.
One could generate many pictures from the first process resembling the first picture in structure, but none would look like any, that were created by the second or third process.
A small spoiler of \cref{sec:random_fields} reveals that the difference in structure is determined by how much points in space are influenced by their neighboring points. 

\begin{figure}[b]
    \centering
    \includegraphics[scale=0.71]{random_field_matern_l_100_n_05-img0.png}
    \includegraphics[scale=0.71]{random_field_matern_l_100_n_100-img0.png}
    \includegraphics[scale=0.71]{random_field_cos_l_1000-img0.png}
    \caption{Samples drawn from random fields with three different covariance functions.}
    \label{fig:random_fields_intro.png}
\end{figure}








\subsection*{Gaussian Process Regression}
Say we now have a random process to model the structure of our optimization problem, more specifically a Gaussian process. It is characterized by a mean and covariance function quantifying the random structure. 
Possible realizations of such a process are shown in \cref{fig:sausage_plot_prior_matern}.
In Bayesian terms, this model is the prior, so the likelihood of samples before incorporating any measurement data.
How do we ensure, that the random process meets the measured values at the points of observation? 
For those points, how do we make it not random?
This is where the regression framework comes in. 
We can construct another random process, one that is an optimal predictor of the underlying process, conditional on the measured values.
It is constructed as a weighted linear combination of the measured values, with a covariance function that becomes zero at points of measurement. 
In Bayesian terms, this is the posterior.
In this way we can frame our beliefs as a prior distribution and update our model by conditioning on observations, thus arriving at a posterior distribution like \cref{fig:sausage_plot_posterior_matern}. 
This model is often referred to as a surrogate model.
One benefit of choosing Gaussian processes is that such a construction is then also a Gaussian process. 

% For a brief practical review of Bayesian statistics, we refer to~\cite{fornacon2022understanding}. \textcolor{red}{Maybe I leave out this last sentence.}

\begin{figure}[t]
    \centering
    \input{figures/sausage_plot_prior_matern.pgf}
    \caption{Gaussian process regression prior.}
    \label{fig:sausage_plot_prior_matern}
\end{figure}

Problems like this first arose in the area of geostatistics~\cite{matheron1963principles}.
When mining gold it is of great interest to find out where the greatest probability of a dense deposit of gold is.
Due to the geological structure, i.e. porosity of the ground and other variables, the spatial distibution looks inherently random. 
% The fact, that it is actually deterministic is not of much value, since there is no easily accessible method of inference.
In this setting, the random density of gold at points is partially determined by the density of gold in the proximity.
The governing principle is, that things that are close together are similar. 
Given a measurment of high gold concentration at a certain point, there is a high probability of having a similar concentration 5 meters away. 
But with 50 meters distance there is less certainty.
It is therefore reasonable to search for an optimal place to mine by not just relying on the observations, but also utilizing the mean and covariance functions generated by the Gaussian process regression, like in \cref{fig:sausage_plot_posterior_matern}.

Beyond geostatistics, this model of Gaussian process regression allows for enough abstraction to be useful in many applications. A more recent example is design problems. Designing molecules for a new drug~\cite{korovina2020chembo}, designing structural metal parts for a new airplane wing~\cite{sakata2003structural}, or setting hyperparameters in machine learning applications~\cite{wu2019hyperparameter}. 
In these contexts, the spaces of optimization are parameter spaces, with parameters like material thickness and diameter.
An observation amounts to running a simulation with a set of parameters and seeing how well they perform given a certain objective function accounting for things like structural stability, weight, material cost.  
Whenever observation is expensive and we have information about how things that are close together are similar, Gaussian processes become a viable option.





\subsection*{Bayesian Optimization}
% \textcolor{red}{Things to change in the introduction: Make it slightly more formal and expand on EGO and the mathematical things we'll do once I understand it better. Go into detail about concentration inequalities when I talk about chaining.}
An observant reader might have already noticed that the observation points just appeared without much consideration. 
Realistically, one can often pick them sequentially in an optimal manner.
This leads to a scheme called Bayesian optimization.
Instead of calculating one Gaussian process regression, Bayesian optimization involves building a new Gaussian process surrogate model after each measurement. And then using this model to pick the next observation point.
This is repeated until some stopping criterion is met.
When picking an observation point, there is a trade-off between exploring areas of big uncertainty and exploiting a known low value and searching in its immediate neighborhood for a lower value.
This trade-off can be weighted differently with different objective functions, also called acquisition functions. Thus leading to different optimization schemes, some of which we will discuss in \cref{sec:bayes_opt}.
% One common example is maximizing the expected improvement~\cite{jones1998efficient}.
\begin{figure}[t]
    \centering
    \input{figures/sausage_plot_posterior_matern.pgf}
    \caption{Gaussian process regression with observations.}
    \label{fig:sausage_plot_posterior_matern}
\end{figure}





\subsection*{Chance of Further Improvement in Bayesian Optimization}
The primary contribution of this thesis is the derivation of an upper bound on the chance of further improvement in a given area during Bayesian optimization.
In order to derive such a bound, we will use chaining tools.
Roughly speaking these are tools that use the metric entropy of the data generated by the random process to show that it has certain regularity properties.
With this bound on the chance of further improvement, one can stop searching for solutions in areas where further improvement is unlikely.
We will show an example of an implementation in an algorithm similar to the branch and bound algorithm briefly mentioned earlier.





\section{Related work}
The review papers~\cite{shahriari2015taking,frazier2018tutorial} give a good overview of recent work in the area of Bayesian optimization.
Some worthwhile mentions are the works of~\cite{auer2002using,srinivas2009gaussian}, which use an upper confidence bound as an acquisition function for Bayesian optimization. 
The works by~\cite{munos2011optimistic,wang2014bayesian} build on this to create an optimistic optimization algorithm, which can be extended into a branch and bound scheme~\cite{de2012exponential}. Also worth mentioning is~\cite{contal2015optimization}, where chaining techniques were used for Bayesian optimization.
% The figures in this thesis were created with the help of the python packages~\cite{scikit-learn, bayesopt-figures, cadiou2022fyeld}. Some credit for the figure designs goes to~\cite{garnett2023bayesian, durrande2021gps}.








\section{Structure of this Thesis}
The rest of the thesis is structured as follows.
In \cref{chap:foundations} we review the necessary probability theory foundations. We define the basics of measure theory and introduce random variables. Then we go on to define stochastic processes and random fields. We explain concentration and chaining, which are tools that help us uniformly bound stochastic processes.
In \cref{chap:kriging_bayes} we explain Gaussian process regression and Bayesian optimization in detail. For the former, we show that it is the best linear unbiased predictor. For the latter, we give a brief overview and talk about different quantities one can optimize for.
In \cref{chap:bound} we then derive a bound of the metric entropy of a stochastic process obtained from Bayesian optimization. This bound is then used to derive an upper bound on the chance of further improvement by two different approaches. Firstly using Dudley's inequality combined with a concentration inequality. And secondly using Talagrand's inequality.
This bound is then implemented in an algorithm.
We end the thesis with a quick conclusion of the results and an outlook in \cref{chap:conclusion_outlook}.
If any notation is not self evident from the context, we refer to \cref{appsec:notation}.